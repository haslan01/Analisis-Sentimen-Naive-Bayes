{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bb0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117e1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_positive = 1 \n",
    "label_negative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a82483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Komentar</th>\n",
       "      <th>Label</th>\n",
       "      <th>TOKENIZATION</th>\n",
       "      <th>STOP_REMOVAL</th>\n",
       "      <th>STEMMER</th>\n",
       "      <th>Komentar_Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>seharusnya ruu tentang perlindungan korban kek...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['seharusnya', 'ruu', 'tentang', 'perlindungan...</td>\n",
       "      <td>['ruu', 'perlindungan', 'korban', 'kekerasan',...</td>\n",
       "      <td>['ruu', 'lindung', 'korban', 'keras', 'seksual...</td>\n",
       "      <td>ruu lindung korban keras seksual hrs legal byk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bahkan sekelas goto aja masih kelolosan apalag...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['bahkan', 'sekelas', 'goto', 'aja', 'masih', ...</td>\n",
       "      <td>['sekelas', 'goto', 'kelolosan', 'rempahan', '...</td>\n",
       "      <td>['kelas', 'goto', 'lolos', 'rempah', 'renggina...</td>\n",
       "      <td>kelas goto lolos rempah rengginang kong wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mendikbud yg tdk faham adab bangsa timur</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['mendikbud', 'yg', 'tdk', 'faham', 'adab', 'b...</td>\n",
       "      <td>['mendikbud', 'tdk', 'faham', 'adab', 'bangsa'...</td>\n",
       "      <td>['mendikbud', 'tdk', 'faham', 'adab', 'bangsa'...</td>\n",
       "      <td>mendikbud tdk faham adab bangsa timur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>kirain digugat gara gara data mining</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['kirain', 'digugat', 'gara', 'gara', 'data', ...</td>\n",
       "      <td>['kirain', 'digugat', 'gara', 'gara', 'data', ...</td>\n",
       "      <td>['kirain', 'gugat', 'gara', 'gara', 'data', 'm...</td>\n",
       "      <td>kirain gugat gara gara data mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>narasi suka sama yg bebas</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['narasi', 'suka', 'sama', 'yg', 'bebas']</td>\n",
       "      <td>['narasi', 'suka', 'bebas']</td>\n",
       "      <td>['narasi', 'suka', 'bebas']</td>\n",
       "      <td>narasi suka bebas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Komentar    Label  \\\n",
       "0           0  seharusnya ruu tentang perlindungan korban kek...  negatif   \n",
       "1           1  bahkan sekelas goto aja masih kelolosan apalag...  negatif   \n",
       "2           2           mendikbud yg tdk faham adab bangsa timur  negatif   \n",
       "3           3              kirain digugat gara gara data mining   negatif   \n",
       "4           4                          narasi suka sama yg bebas  negatif   \n",
       "\n",
       "                                        TOKENIZATION  \\\n",
       "0  ['seharusnya', 'ruu', 'tentang', 'perlindungan...   \n",
       "1  ['bahkan', 'sekelas', 'goto', 'aja', 'masih', ...   \n",
       "2  ['mendikbud', 'yg', 'tdk', 'faham', 'adab', 'b...   \n",
       "3  ['kirain', 'digugat', 'gara', 'gara', 'data', ...   \n",
       "4          ['narasi', 'suka', 'sama', 'yg', 'bebas']   \n",
       "\n",
       "                                        STOP_REMOVAL  \\\n",
       "0  ['ruu', 'perlindungan', 'korban', 'kekerasan',...   \n",
       "1  ['sekelas', 'goto', 'kelolosan', 'rempahan', '...   \n",
       "2  ['mendikbud', 'tdk', 'faham', 'adab', 'bangsa'...   \n",
       "3  ['kirain', 'digugat', 'gara', 'gara', 'data', ...   \n",
       "4                        ['narasi', 'suka', 'bebas']   \n",
       "\n",
       "                                             STEMMER  \\\n",
       "0  ['ruu', 'lindung', 'korban', 'keras', 'seksual...   \n",
       "1  ['kelas', 'goto', 'lolos', 'rempah', 'renggina...   \n",
       "2  ['mendikbud', 'tdk', 'faham', 'adab', 'bangsa'...   \n",
       "3  ['kirain', 'gugat', 'gara', 'gara', 'data', 'm...   \n",
       "4                        ['narasi', 'suka', 'bebas']   \n",
       "\n",
       "                                      Komentar_Final  \n",
       "0  ruu lindung korban keras seksual hrs legal byk...  \n",
       "1        kelas goto lolos rempah rengginang kong wan  \n",
       "2              mendikbud tdk faham adab bangsa timur  \n",
       "3                 kirain gugat gara gara data mining  \n",
       "4                                  narasi suka bebas  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'PreProcessing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495b50c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Komentar</th>\n",
       "      <th>Label</th>\n",
       "      <th>TOKENIZATION</th>\n",
       "      <th>STEMMER</th>\n",
       "      <th>Komentar_Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seharusnya ruu tentang perlindungan korban kek...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['seharusnya', 'ruu', 'tentang', 'perlindungan...</td>\n",
       "      <td>['ruu', 'lindung', 'korban', 'keras', 'seksual...</td>\n",
       "      <td>ruu lindung korban keras seksual hrs legal byk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bahkan sekelas goto aja masih kelolosan apalag...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['bahkan', 'sekelas', 'goto', 'aja', 'masih', ...</td>\n",
       "      <td>['kelas', 'goto', 'lolos', 'rempah', 'renggina...</td>\n",
       "      <td>kelas goto lolos rempah rengginang kong wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mendikbud yg tdk faham adab bangsa timur</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['mendikbud', 'yg', 'tdk', 'faham', 'adab', 'b...</td>\n",
       "      <td>['mendikbud', 'tdk', 'faham', 'adab', 'bangsa'...</td>\n",
       "      <td>mendikbud tdk faham adab bangsa timur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kirain digugat gara gara data mining</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['kirain', 'digugat', 'gara', 'gara', 'data', ...</td>\n",
       "      <td>['kirain', 'gugat', 'gara', 'gara', 'data', 'm...</td>\n",
       "      <td>kirain gugat gara gara data mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>narasi suka sama yg bebas</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['narasi', 'suka', 'sama', 'yg', 'bebas']</td>\n",
       "      <td>['narasi', 'suka', 'bebas']</td>\n",
       "      <td>narasi suka bebas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Komentar    Label  \\\n",
       "0  seharusnya ruu tentang perlindungan korban kek...  negatif   \n",
       "1  bahkan sekelas goto aja masih kelolosan apalag...  negatif   \n",
       "2           mendikbud yg tdk faham adab bangsa timur  negatif   \n",
       "3              kirain digugat gara gara data mining   negatif   \n",
       "4                          narasi suka sama yg bebas  negatif   \n",
       "\n",
       "                                        TOKENIZATION  \\\n",
       "0  ['seharusnya', 'ruu', 'tentang', 'perlindungan...   \n",
       "1  ['bahkan', 'sekelas', 'goto', 'aja', 'masih', ...   \n",
       "2  ['mendikbud', 'yg', 'tdk', 'faham', 'adab', 'b...   \n",
       "3  ['kirain', 'digugat', 'gara', 'gara', 'data', ...   \n",
       "4          ['narasi', 'suka', 'sama', 'yg', 'bebas']   \n",
       "\n",
       "                                             STEMMER  \\\n",
       "0  ['ruu', 'lindung', 'korban', 'keras', 'seksual...   \n",
       "1  ['kelas', 'goto', 'lolos', 'rempah', 'renggina...   \n",
       "2  ['mendikbud', 'tdk', 'faham', 'adab', 'bangsa'...   \n",
       "3  ['kirain', 'gugat', 'gara', 'gara', 'data', 'm...   \n",
       "4                        ['narasi', 'suka', 'bebas']   \n",
       "\n",
       "                                      Komentar_Final  \n",
       "0  ruu lindung korban keras seksual hrs legal byk...  \n",
       "1        kelas goto lolos rempah rengginang kong wan  \n",
       "2              mendikbud tdk faham adab bangsa timur  \n",
       "3                 kirain gugat gara gara data mining  \n",
       "4                                  narasi suka bebas  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df.columns[[0, 4]], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ca44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test data\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(df['Komentar_Final'], df['Label'], test_size = 0.2,random_state = 20)\n",
    "# random_state = 20 menyatakan kita akan mendapatkan output yang sama dengan saat pertama kali membuat pemisahan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05814c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_train['Sentiment'] = train_X\n",
    "df_train['Label'] = train_Y\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['Sentiment'] = test_X\n",
    "df_test['Label'] = test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b46d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(label):\n",
    "  if label == 'positif':\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "df_train['Label'] = train_Y.apply(convert)\n",
    "df_test['Label'] = test_Y.apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40fdf811",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m tfidf_vect \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# tfidf_vect.fit(df_train['Sentiment'])\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtfidf_vect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKomentar_Final\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2049\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2044\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2045\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2046\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2047\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2048\u001b[0m )\n\u001b[1;32m-> 2049\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2050\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:234\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    231\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(max_features = 5000)\n",
    "# tfidf_vect.fit(df_train['Sentiment'])\n",
    "tfidf_vect.fit(df['Komentar_Final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"selected words as feature : \")\n",
    "print(\"----------------------------\")\n",
    "print(tfidf_vect.get_feature_names())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the below syntax to see the vocabulary that it has learned from the corpus\n",
    "print(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d93d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"jumlah data training : \")\n",
    "print(len(train_X))\n",
    "print()\n",
    "\n",
    "print(\"jumlah data test : \") \n",
    "print(len(test_X))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tfidf = tfidf_vect.transform(df_train['Sentiment'])\n",
    "test_X_tfidf = tfidf_vect.transform(df_test['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7471634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_mat = tfidf_vect.transform(df['Komentar_Final']).toarray()\n",
    "tfidf_mat = tfidf_vect.transform(df_train['Sentiment']).toarray()\n",
    "# tfidf_mat = tfidf_vect.transform(df_test['Sentiment']).toarray()\n",
    "\n",
    "terms = tfidf_vect.get_feature_names()\n",
    "\n",
    "# menjumlahkan tfidf dari tiap kata/term di semua dataset\n",
    "sums = tfidf_mat.sum(axis=0)\n",
    "\n",
    "# menampilkan jumlah tfidf dari tiap kata yang ada di dataset\n",
    "data = []\n",
    "for col, term in enumerate(terms):\n",
    "    data.append((term, sums[col] ))\n",
    "\n",
    "ranking = pd.DataFrame(data, columns=['term','TF-IDF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_br=ranking.sort_values('TF-IDF', ascending=False)\n",
    "print(ranking_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_br.to_csv(r'tfidf_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "model.fit(train_X_tfidf,df_train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e17300",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_X_tfidf)\n",
    "acc = (accuracy_score(df_test['Label'],predict))*100\n",
    "\n",
    "print(round(acc,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1733bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(df_test['Label'], predict)\n",
    "print(\"Confusion Matrix : \") \n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "f, ax = plt.subplots(figsize=(8,5))\n",
    "sns.heatmap(confusion_matrix(df_test['Label'], predict), annot=True, fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"predict\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.savefig(r\"confusion matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Accuracy, Precision, Recall, f1-score\n",
    "print (\"\\nHere is the classification report:\") \n",
    "print (classification_report(df_test['Label'], predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6590ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghitung jumlah label positif dan negatif pada data test setelah hasil prediksi model\n",
    "test_after_nb_count_label = collections.Counter(predict)\n",
    "juml_pos_nb= test_after_nb_count_label[label_positive]\n",
    "juml_neg_nb = test_after_nb_count_label[label_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie chart analisis sentimen\n",
    "labels = ['Positive','Negative']\n",
    "Category = [juml_pos_nb, juml_neg_nb]\n",
    "fig, ax = plt.subplots()\n",
    "color = ['blue', 'red']\n",
    "plt.pie(Category, labels=labels, colors=color,startangle=90, shadow=True, autopct='%1.2f%%', explode=(0.1, 0))\n",
    "plt.title('Diagram Lingkar Data Hasil Prediksi Klasifikasi Naive Bayes')\n",
    "plt.legend()\n",
    "plt.savefig(r\"pie_nb.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "kamus_kata = pd.read_csv(r'tfidf_train.csv')\n",
    "kamus_kata.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a94feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del kamus_kata['Unnamed: 0']\n",
    "kamus_kata.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model,\n",
    "            open('model_nb.pkl', 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f34e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_vect,\n",
    "            open('tfidf.pkl', 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bb6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
