{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc29cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c2d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install swifter\n",
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c62097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "import csv\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9100e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'pelabelan-gabung.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning Text\n",
    "def remove_punct(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', str(text))\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = re.sub(r'\\b\\w(1,2)\\b', '', text)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    return text\n",
    "df['Komentar'] = df['Komentar'].apply(remove_punct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50227a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "df['Komentar'] = df['Komentar'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a902a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_repeat_character(text):\n",
    "    # Pattern to look for three or more repetitions of any character, including newlines (contoh goool -> gol).\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "df['Komentar'] = df['Komentar'].apply(replace_repeat_character) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d265a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "    \n",
    "df['TOKENIZATION'] = df['Komentar'].apply(tokenize)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce787fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "katabaku = csv.reader(\n",
    "\topen(r\"kata_baku.csv\"), delimiter=\";\") # ambil file csv kata baku menjadi array\n",
    "\n",
    "kamus_katabaku = {} # empty dictionary untuk kamus kata baku\n",
    "try:\n",
    "    for row in katabaku : # membuat kamus kata baku dengan input kata tidak baku dan output kata bakunya\n",
    "        kamus_katabaku[row[0]] = row[1]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f798caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kamus_katabaku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10194b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_term(text):\n",
    "    return [kamus_katabaku[term] if term in kamus_katabaku else term for term in text]\n",
    "df['TOKENIZATION'] = df['TOKENIZATION'].apply(normalized_term) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword =nltk.corpus.stopwords.words('indonesian')\n",
    "stopword.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                 'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                 'krn', 'nya', 'nih', 'sih', \n",
    "                 'si', 'tau', 'tuh', 'utk', 'ya','sdh',\n",
    "                 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                 '&amp', 'yah', 'njir', 'wkwk', 'haha', 'hehe', 'rs', 'yaa',\n",
    "                 'nii', 'bin', 'lahh', 'hmm', 'lieur', 'njarem', 'kale',\n",
    "                 'hd', 'an', 'ob', 'bro', 'ud', 'iyaa', 'guys', 'doang',\n",
    "                 'doank', 'eh', 'kek', 'or', 'kah', 'eso', 'tuh', 'dh', 'dnk',\n",
    "                 'mh', 'ja', 'tu', 'anu', 'ni', 'ku', 'blz', 'gy', 'skg', 'eeh',\n",
    "                 'pas','tar','bc','x','dong','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=['enggak','tidak','jangan','belum']\n",
    "stopword = [word for word in stopword if word not in word_list] #make a copy of the word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefdcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stopword]\n",
    "    \n",
    "df['STOP_REMOVAL'] = df['TOKENIZATION'].apply(remove_stopwords)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "import swifter\n",
    "\n",
    "# implementasi pada data kita\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "text_dict = {}\n",
    "\n",
    "for document in df['STOP_REMOVAL']:\n",
    "    for text in document:\n",
    "        if text not in text_dict:\n",
    "            text_dict[text] = ' '\n",
    "            \n",
    "print(len(text_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for text in text_dict:\n",
    "    text_dict[text] = stemmed_wrapper(text)\n",
    "    print(text,\":\" ,text_dict[text])\n",
    "    \n",
    "print(text_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [text_dict[text] for text in document]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb543384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STEMMER'] = df['STOP_REMOVAL'].apply(get_stemmed_term)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stemmer(text):\n",
    "     text = np.array(text)\n",
    "     text = ' '.join(text)\n",
    "     return text\n",
    "\n",
    "df['Komentar_Final'] = df['STEMMER'].apply(lambda x: fit_stemmer(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d544de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'PreProcessingnajwa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(stopword,\n",
    "            open('stopword.pkl', 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8aeb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(kamus_katabaku,\n",
    "            open('kamus_katabaku.pkl', 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fed42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
