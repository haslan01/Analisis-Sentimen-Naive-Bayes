{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc29cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.3.2 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from pandas==1.3.2) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from pandas==1.3.2) (1.23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from pandas==1.3.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from python-dateutil>=2.7.3->pandas==1.3.2) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190c2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: bleach>=3.1.1 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from swifter) (5.0.1)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from swifter) (2022.8.1)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from swifter) (2.1.0)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from swifter) (5.9.1)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from swifter) (4.64.0)\n",
      "Requirement already satisfied: parso>0.4.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from swifter) (0.8.3)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from swifter) (1.3.2)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from swifter) (8.0.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from bleach>=3.1.1->swifter) (1.16.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (21.3)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2022.7.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from dask[dataframe]>=2.10.0->swifter) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.23.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (8.4.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (6.15.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (3.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (5.3.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (4.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from pandas>=1.0.0->swifter) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.5)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (23.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.6.3)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.5)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.1.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (7.3.4)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (2.13.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (63.4.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.4.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (3.0.30)\n",
      "Requirement already satisfied: backcall in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.18.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from packaging>=20.0->dask[dataframe]>=2.10.0->swifter) (3.0.9)\n",
      "Requirement already satisfied: locket in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (4.11.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.5)\n",
      "Requirement already satisfied: asttokens in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (2.0.8)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.2.2)\n",
      "Requirement already satisfied: executing in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=7.0.0->swifter) (0.10.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (227)\n",
      "Requirement already satisfied: Sastrawi in c:\\users\\asus\\.conda\\envs\\aslan\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install swifter\n",
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c62097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "import csv\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9100e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r'label-gabung.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning Text\n",
    "def remove_punct(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', str(text))\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    text = re.sub(r'\\b\\w(1,2)\\b', '', text)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    return text\n",
    "df['Komentar'] = df['Komentar'].apply(remove_punct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50227a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "df['Komentar'] = df['Komentar'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a902a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_repeat_character(text):\n",
    "    # Pattern to look for three or more repetitions of any character, including newlines (contoh goool -> gol).\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "df['Komentar'] = df['Komentar'].apply(replace_repeat_character) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d265a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "    \n",
    "df['TOKENIZATION'] = df['Komentar'].apply(tokenize)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce787fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "katabaku = csv.reader(\n",
    "\topen(r\"kata_baku.csv\"), delimiter=\";\") # ambil file csv kata baku menjadi array\n",
    "\n",
    "kamus_katabaku = {} # empty dictionary untuk kamus kata baku\n",
    "try:\n",
    "    for row in katabaku : # membuat kamus kata baku dengan input kata tidak baku dan output kata bakunya\n",
    "        kamus_katabaku[row[0]] = row[1]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f798caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kamus_katabaku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10194b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_term(text):\n",
    "    return [kamus_katabaku[term] if term in kamus_katabaku else term for term in text]\n",
    "df['TOKENIZATION'] = df['TOKENIZATION'].apply(normalized_term) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword =nltk.corpus.stopwords.words('indonesian')\n",
    "stopword.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                 'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                 'krn', 'nya', 'nih', 'sih', \n",
    "                 'si', 'tau', 'tuh', 'utk', 'ya','sdh',\n",
    "                 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                 '&amp', 'yah', 'njir', 'wkwk', 'haha', 'hehe', 'rs', 'yaa',\n",
    "                 'nii', 'bin', 'lahh', 'hmm', 'lieur', 'njarem', 'kale',\n",
    "                 'hd', 'an', 'ob', 'bro', 'ud', 'iyaa', 'guys', 'doang',\n",
    "                 'doank', 'eh', 'kek', 'or', 'kah', 'eso', 'tuh', 'dh', 'dnk',\n",
    "                 'mh', 'ja', 'tu', 'anu', 'ni', 'ku', 'blz', 'gy', 'skg', 'eeh',\n",
    "                 'pas','tar','bc','x','dong','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=['enggak','tidak','jangan','belum']\n",
    "stopword = [word for word in stopword if word not in word_list] #make a copy of the word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2398d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefdcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stopword]\n",
    "    \n",
    "df['STOP_REMOVAL'] = df['TOKENIZATION'].apply(remove_stopwords)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087b362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "import swifter\n",
    "\n",
    "# implementasi pada data kita\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "text_dict = {}\n",
    "\n",
    "for document in df['STOP_REMOVAL']:\n",
    "    for text in document:\n",
    "        if text not in text_dict:\n",
    "            text_dict[text] = ' '\n",
    "            \n",
    "print(len(text_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for text in text_dict:\n",
    "    text_dict[text] = stemmed_wrapper(text)\n",
    "    print(text,\":\" ,text_dict[text])\n",
    "    \n",
    "print(text_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [text_dict[text] for text in document]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb543384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STEMMER'] = df['STOP_REMOVAL'].apply(get_stemmed_term)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stemmer(text):\n",
    "     text = np.array(text)\n",
    "     text = ' '.join(text)\n",
    "     return text\n",
    "\n",
    "df['Komentar_Final'] = df['STEMMER'].apply(lambda x: fit_stemmer(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d544de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'PreProcessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(stopword,\n",
    "            open('stopword.pkl', 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8aeb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(kamus_katabaku,\n",
    "            open('kamus_katabaku.pkl', 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fed42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
