{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813297a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1376fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_positive = 1 \n",
    "label_netral = 0\n",
    "label_negative = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc132fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Komentar</th>\n",
       "      <th>Label</th>\n",
       "      <th>TOKENIZATION</th>\n",
       "      <th>STOP_REMOVAL</th>\n",
       "      <th>STEMMER</th>\n",
       "      <th>Komentar_Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lha kekerasan seksual itu kan mmg definisinya ...</td>\n",
       "      <td>positif</td>\n",
       "      <td>['lha', 'kekerasan', 'seksual', 'itu', 'kan', ...</td>\n",
       "      <td>['lha', 'kekerasan', 'seksual', 'mmg', 'defini...</td>\n",
       "      <td>['lha', 'keras', 'seksual', 'mmg', 'definisi',...</td>\n",
       "      <td>lha keras seksual mmg definisi hrs tuju korban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>jangan mendekati zina itu adalah aturan yang p...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['jngan', 'mendekati', 'zinah', 'itu', 'adalah...</td>\n",
       "      <td>['jngan', 'mendekati', 'zinah', 'aturan', 'dn'...</td>\n",
       "      <td>['jngan', 'dekat', 'zinah', 'atur', 'dn', 'nda...</td>\n",
       "      <td>jngan dekat zinah atur dn ndak pasal uu dll de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the law is an art semakin rinci semakin mudah ...</td>\n",
       "      <td>positif</td>\n",
       "      <td>['the', 'law', 'is', 'an', 'art', 'semakin', '...</td>\n",
       "      <td>['the', 'law', 'is', 'art', 'rinci', 'gampang'...</td>\n",
       "      <td>['the', 'law', 'is', 'art', 'rinci', 'gampang'...</td>\n",
       "      <td>the law is art rinci gampang belok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tetapkan dulu apa yang dimaksud dengan kekeras...</td>\n",
       "      <td>netral</td>\n",
       "      <td>['tetapkan', 'dulu', 'aoa', 'yg', 'dimaksud', ...</td>\n",
       "      <td>['tetapkan', 'aoa', 'kekerasan', 'seksualitas']</td>\n",
       "      <td>['tetap', 'aoa', 'keras', 'seksualitas']</td>\n",
       "      <td>tetap aoa keras seksualitas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>lbih cock diterapkan dikemendikbud dan dpr</td>\n",
       "      <td>netral</td>\n",
       "      <td>['lbih', 'cock', 'diterapkan', 'dikemendikbud'...</td>\n",
       "      <td>['lbih', 'cock', 'diterapkan', 'dikemendikbud'...</td>\n",
       "      <td>['lbih', 'cock', 'terap', 'dikemendikbud', 'dn...</td>\n",
       "      <td>lbih cock terap dikemendikbud dn dpr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Komentar    Label  \\\n",
       "0           0  lha kekerasan seksual itu kan mmg definisinya ...  positif   \n",
       "1           1  jangan mendekati zina itu adalah aturan yang p...  negatif   \n",
       "2           2  the law is an art semakin rinci semakin mudah ...  positif   \n",
       "3           3  tetapkan dulu apa yang dimaksud dengan kekeras...   netral   \n",
       "4           4        lbih cock diterapkan dikemendikbud dan dpr    netral   \n",
       "\n",
       "                                        TOKENIZATION  \\\n",
       "0  ['lha', 'kekerasan', 'seksual', 'itu', 'kan', ...   \n",
       "1  ['jngan', 'mendekati', 'zinah', 'itu', 'adalah...   \n",
       "2  ['the', 'law', 'is', 'an', 'art', 'semakin', '...   \n",
       "3  ['tetapkan', 'dulu', 'aoa', 'yg', 'dimaksud', ...   \n",
       "4  ['lbih', 'cock', 'diterapkan', 'dikemendikbud'...   \n",
       "\n",
       "                                        STOP_REMOVAL  \\\n",
       "0  ['lha', 'kekerasan', 'seksual', 'mmg', 'defini...   \n",
       "1  ['jngan', 'mendekati', 'zinah', 'aturan', 'dn'...   \n",
       "2  ['the', 'law', 'is', 'art', 'rinci', 'gampang'...   \n",
       "3    ['tetapkan', 'aoa', 'kekerasan', 'seksualitas']   \n",
       "4  ['lbih', 'cock', 'diterapkan', 'dikemendikbud'...   \n",
       "\n",
       "                                             STEMMER  \\\n",
       "0  ['lha', 'keras', 'seksual', 'mmg', 'definisi',...   \n",
       "1  ['jngan', 'dekat', 'zinah', 'atur', 'dn', 'nda...   \n",
       "2  ['the', 'law', 'is', 'art', 'rinci', 'gampang'...   \n",
       "3           ['tetap', 'aoa', 'keras', 'seksualitas']   \n",
       "4  ['lbih', 'cock', 'terap', 'dikemendikbud', 'dn...   \n",
       "\n",
       "                                      Komentar_Final  \n",
       "0  lha keras seksual mmg definisi hrs tuju korban...  \n",
       "1  jngan dekat zinah atur dn ndak pasal uu dll de...  \n",
       "2                 the law is art rinci gampang belok  \n",
       "3                        tetap aoa keras seksualitas  \n",
       "4               lbih cock terap dikemendikbud dn dpr  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'PreProcessing.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc764b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Komentar</th>\n",
       "      <th>Label</th>\n",
       "      <th>TOKENIZATION</th>\n",
       "      <th>STEMMER</th>\n",
       "      <th>Komentar_Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lha kekerasan seksual itu kan mmg definisinya ...</td>\n",
       "      <td>positif</td>\n",
       "      <td>['lha', 'kekerasan', 'seksual', 'itu', 'kan', ...</td>\n",
       "      <td>['lha', 'keras', 'seksual', 'mmg', 'definisi',...</td>\n",
       "      <td>lha keras seksual mmg definisi hrs tuju korban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jangan mendekati zina itu adalah aturan yang p...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>['jngan', 'mendekati', 'zinah', 'itu', 'adalah...</td>\n",
       "      <td>['jngan', 'dekat', 'zinah', 'atur', 'dn', 'nda...</td>\n",
       "      <td>jngan dekat zinah atur dn ndak pasal uu dll de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the law is an art semakin rinci semakin mudah ...</td>\n",
       "      <td>positif</td>\n",
       "      <td>['the', 'law', 'is', 'an', 'art', 'semakin', '...</td>\n",
       "      <td>['the', 'law', 'is', 'art', 'rinci', 'gampang'...</td>\n",
       "      <td>the law is art rinci gampang belok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tetapkan dulu apa yang dimaksud dengan kekeras...</td>\n",
       "      <td>netral</td>\n",
       "      <td>['tetapkan', 'dulu', 'aoa', 'yg', 'dimaksud', ...</td>\n",
       "      <td>['tetap', 'aoa', 'keras', 'seksualitas']</td>\n",
       "      <td>tetap aoa keras seksualitas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lbih cock diterapkan dikemendikbud dan dpr</td>\n",
       "      <td>netral</td>\n",
       "      <td>['lbih', 'cock', 'diterapkan', 'dikemendikbud'...</td>\n",
       "      <td>['lbih', 'cock', 'terap', 'dikemendikbud', 'dn...</td>\n",
       "      <td>lbih cock terap dikemendikbud dn dpr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Komentar    Label  \\\n",
       "0  lha kekerasan seksual itu kan mmg definisinya ...  positif   \n",
       "1  jangan mendekati zina itu adalah aturan yang p...  negatif   \n",
       "2  the law is an art semakin rinci semakin mudah ...  positif   \n",
       "3  tetapkan dulu apa yang dimaksud dengan kekeras...   netral   \n",
       "4        lbih cock diterapkan dikemendikbud dan dpr    netral   \n",
       "\n",
       "                                        TOKENIZATION  \\\n",
       "0  ['lha', 'kekerasan', 'seksual', 'itu', 'kan', ...   \n",
       "1  ['jngan', 'mendekati', 'zinah', 'itu', 'adalah...   \n",
       "2  ['the', 'law', 'is', 'an', 'art', 'semakin', '...   \n",
       "3  ['tetapkan', 'dulu', 'aoa', 'yg', 'dimaksud', ...   \n",
       "4  ['lbih', 'cock', 'diterapkan', 'dikemendikbud'...   \n",
       "\n",
       "                                             STEMMER  \\\n",
       "0  ['lha', 'keras', 'seksual', 'mmg', 'definisi',...   \n",
       "1  ['jngan', 'dekat', 'zinah', 'atur', 'dn', 'nda...   \n",
       "2  ['the', 'law', 'is', 'art', 'rinci', 'gampang'...   \n",
       "3           ['tetap', 'aoa', 'keras', 'seksualitas']   \n",
       "4  ['lbih', 'cock', 'terap', 'dikemendikbud', 'dn...   \n",
       "\n",
       "                                      Komentar_Final  \n",
       "0  lha keras seksual mmg definisi hrs tuju korban...  \n",
       "1  jngan dekat zinah atur dn ndak pasal uu dll de...  \n",
       "2                 the law is art rinci gampang belok  \n",
       "3                        tetap aoa keras seksualitas  \n",
       "4               lbih cock terap dikemendikbud dn dpr  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df.columns[[0, 4]], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255f5c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test data\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = model_selection.train_test_split(df['Komentar_Final'], df['Label'], test_size = 0.2,random_state = 20)\n",
    "# random_state = 20 menyatakan kita akan mendapatkan output yang sama dengan saat pertama kali membuat pemisahan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5062ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_train['Sentiment'] = train_X\n",
    "df_train['Label'] = train_Y\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['Sentiment'] = test_X\n",
    "df_test['Label'] = test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91af29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(label):\n",
    "  if label == 'positif':\n",
    "    return 1\n",
    "  elif label == 'netral':\n",
    "    return 0\n",
    "  else:\n",
    "    return -1\n",
    "\n",
    "df_train['Label'] = train_Y.apply(convert)\n",
    "df_test['Label'] = test_Y.apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f72922b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m tfidf_vect \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# tfidf_vect.fit(df_train['Sentiment'])\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtfidf_vect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKomentar_Final\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2049\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2044\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2045\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2046\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2047\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2048\u001b[0m )\n\u001b[1;32m-> 2049\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2050\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\.conda\\envs\\aslan\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:234\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    231\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(max_features = 5000)\n",
    "# tfidf_vect.fit(df_train['Sentiment'])\n",
    "tfidf_vect.fit(df['Komentar_Final'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d60b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbc5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"selected words as feature : \")\n",
    "print(\"----------------------------\")\n",
    "print(tfidf_vect.get_feature_names())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af60293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the below syntax to see the vocabulary that it has learned from the corpus\n",
    "print(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"jumlah data training : \")\n",
    "print(len(train_X))\n",
    "print()\n",
    "\n",
    "print(\"jumlah data test : \") \n",
    "print(len(test_X))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db705eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tfidf = tfidf_vect.transform(df_train['Sentiment'])\n",
    "test_X_tfidf = tfidf_vect.transform(df_test['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b91766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_mat = tfidf_vect.transform(df['Komentar_Final']).toarray()\n",
    "tfidf_mat = tfidf_vect.transform(df_train['Sentiment']).toarray()\n",
    "# tfidf_mat = tfidf_vect.transform(df_test['Sentiment']).toarray()\n",
    "\n",
    "terms = tfidf_vect.get_feature_names()\n",
    "\n",
    "# menjumlahkan tfidf dari tiap kata/term di semua dataset\n",
    "sums = tfidf_mat.sum(axis=0)\n",
    "\n",
    "# menampilkan jumlah tfidf dari tiap kata yang ada di dataset\n",
    "data = []\n",
    "for col, term in enumerate(terms):\n",
    "    data.append((term, sums[col] ))\n",
    "\n",
    "ranking = pd.DataFrame(data, columns=['term','TF-IDF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca63398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_br=ranking.sort_values('TF-IDF', ascending=False)\n",
    "print(ranking_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe677181",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_br.to_csv(r'tfidf_traintiga.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "model.fit(train_X_tfidf,df_train['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c28531",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(test_X_tfidf)\n",
    "acc = (accuracy_score(df_test['Label'],predict))*100\n",
    "\n",
    "print(round(acc,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(df_test['Label'], predict)\n",
    "print(\"Confusion Matrix : \") \n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "f, ax = plt.subplots(figsize=(8,5))\n",
    "sns.heatmap(confusion_matrix(df_test['Label'], predict), annot=True, fmt=\".0f\",ax=ax)\n",
    "plt.xlabel(\"predict\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.savefig(r\"confusion matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30104e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Accuracy, Precision, Recall, f1-score\n",
    "print (\"\\nHere is the classification report:\") \n",
    "print (classification_report(df_test['Label'], predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghitung jumlah label positif dan negatif pada data test setelah hasil prediksi model\n",
    "test_after_nb_count_label = collections.Counter(predict)\n",
    "juml_pos_nb= test_after_nb_count_label[label_positive]\n",
    "juml_net_nb= test_after_nb_count_label[label_netral]\n",
    "juml_neg_nb = test_after_nb_count_label[label_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie chart analisis sentimen\n",
    "labels = ['Positive','Netral','Negative']\n",
    "Category = [juml_pos_nb,juml_net_nb, juml_neg_nb]\n",
    "fig, ax = plt.subplots()\n",
    "color = ['blue','green', 'red']\n",
    "plt.pie(Category, labels=labels, colors=color,startangle=90, shadow=True, autopct='%1.2f%%', explode=(0.1, 0, 0.1))\n",
    "plt.title('Diagram Lingkar Data Hasil Prediksi Klasifikasi Naive Bayes')\n",
    "plt.legend()\n",
    "plt.savefig(r\"pie_nbdtiga.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f480d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kamus_kata = pd.read_csv(r'tfidf_traintiga.csv')\n",
    "kamus_kata.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bbf312",
   "metadata": {},
   "outputs": [],
   "source": [
    "del kamus_kata['Unnamed: 0']\n",
    "kamus_kata.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598217b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model,\n",
    "            open('model_nbtiga.pkl', 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_vect,\n",
    "            open('tfidf.pkl', 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33347876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
